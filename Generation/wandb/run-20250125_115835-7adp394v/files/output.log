 2025-01-25 11:58:38 - Start training from epoch 0
 2025-01-25 11:58:39 - TRAIN - Epoch [0 / 3] - Iter[0 / 8972] - Loss: 135.8152
Training - Epoch [0/3]:   0%|                                                                         | 0/8972 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.






























































































































































Training - Epoch [0/3]:   6%|███▍                                                         | 501/8972 [07:38<3:14:34,  1.38s/it]
































































































































































Training - Epoch [0/3]:  11%|██████▋                                                     | 1002/8972 [15:15<2:38:40,  1.19s/it]
































































































































































Training - Epoch [0/3]:  17%|██████████                                                  | 1500/8972 [22:56<3:46:28,  1.82s/it]































































































































































Training - Epoch [0/3]:  22%|█████████████▎                                              | 2000/8972 [30:35<3:33:25,  1.84s/it]





























































































































































Training - Epoch [0/3]:  28%|█████████████████▎                                            | 2499/8972 [38:09<58:14,  1.85it/s]



































































































































































Training - Epoch [0/3]:  33%|████████████████████                                        | 3001/8972 [45:54<2:24:08,  1.45s/it]






























































































































































Training - Epoch [0/3]:  39%|███████████████████████▍                                    | 3500/8972 [53:34<2:46:21,  1.82s/it]


































































































































































Training - Epoch [0/3]:  45%|█████████████████████████▊                                | 4001/8972 [1:01:13<1:59:04,  1.44s/it]












Training - Epoch [0/3]:  45%|██████████████████████████▏                               | 4044/8972 [1:01:52<1:08:17,  1.20it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fbb5adcdf70>
Traceback (most recent call last):
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1443, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt:
Training - Epoch [0/3]:  45%|██████████████████████████▏                               | 4044/8972 [1:01:52<1:15:24,  1.09it/s]
Traceback (most recent call last):
  File "main.py", line 49, in <module>
    main(args)
  File "main.py", line 40, in main
    job(args)
  File "/home/kyeonghyun/PiFi/Generation/task/summarization/train.py", line 124, in training
    logits = model(src, src_attention_mask, tgt, tgt_attention_mask)
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/kyeonghyun/anaconda3/envs/khyun/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/kyeonghyun/PiFi/Generation/model/summarization/model.py", line 49, in forward
    attention_mask = torch.ones((batch_size, seq_length), dtype=hidden_states.dtype).to(src.device)
KeyboardInterrupt