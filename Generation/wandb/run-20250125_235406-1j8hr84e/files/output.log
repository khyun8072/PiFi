
 2025-01-25 23:54:10 - Loss function: CrossEntropyLoss()
Testing:   0%|                                                                                        | 0/1000 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.




























































































































































































































































































































Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 997/1000 [10:35<00:01,  2.22it/s]

Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [10:36<00:00,  1.57it/s]






























































































































































































































































































































































































































































































TEST - Calculating BERTScore, BARTScore, ROUGE...: 100%|â–‰| 999/1000 [15:57<00:00
 2025-01-26 00:20:51 - TEST - Bleu_1: 0.5841
 2025-01-26 00:20:51 - TEST - Bleu_2: 0.4757
 2025-01-26 00:20:51 - TEST - Bleu_3: 0.3875
 2025-01-26 00:20:51 - TEST - Bleu_4: 0.3162
 2025-01-26 00:20:51 - TEST - Bleu_avg: 0.4409
 2025-01-26 00:20:51 - TEST - Rouge_1: 0.6659
 2025-01-26 00:20:51 - TEST - Rouge_2: 0.4641
 2025-01-26 00:20:51 - TEST - Rouge_L: 0.6386
 2025-01-26 00:20:51 - TEST - Rouge_Lsum: 0.6386
 2025-01-26 00:20:51 - TEST - Rouge_L_NLGEVAL: 0.5728
 2025-01-26 00:20:51 - TEST - Meteor: 0.3178
 2025-01-26 00:20:51 - TEST - BERTScore_Precision: 0.8724
 2025-01-26 00:20:51 - TEST - BERTScore_Recall: 0.8713
 2025-01-26 00:20:51 - TEST - BERTScore_F1: 0.8716

TEST - Calculating BERTScore, BARTScore, ROUGE...: 100%|â–ˆ| 1000/1000 [15:58<00:0
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")